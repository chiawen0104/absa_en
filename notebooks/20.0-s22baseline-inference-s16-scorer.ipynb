{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfc2f0a-050c-420b-9446-ab56c03b8f00",
   "metadata": {},
   "source": [
    "## Scorer: semEval22 baselines, inferences on semEval16 \n",
    "\n",
    "    =================\n",
    "    - Author: Nana \n",
    "    - Created Date: 2022.7.28\n",
    "    =================\n",
    "- models: two semEval22 baseline models (extracts opinion expressions `polarity_expression`\n",
    "- inference dataset: semEval16 ABSA RESTAURANT dataset (with gold answer)\n",
    "- metrics: micro-f1, roughly according to [SemEval-2016 Task 5: Aspect Based Sentiment Analysis](https://aclanthology.org/S16-1002/)\n",
    "- tweaks: \n",
    "    - Because the s22 baselines do not return Aspect Category (e.g. FOOD#QUALITY), we cannot\n",
    "    calc score completely based on the s16 metrics. Furthermore, the micro f1 relies on exact match of strings, i.e. gold: 'wine list' ; pred: 'the wine list' is counted as a false prediction,\n",
    "    which might be improved using string algorithms.\n",
    "    - Because the s16 gold answer does not provide `polarity_expression`, we cannot calc score based using s22 metrics either. \n",
    "    - we'll tweak the scorer function a little bit. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3c726f-e837-46b4-bc15-f002c6baf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../semEval16/data\"\n",
    "seqdir = \"../semEval16/sequence_labeling/predictions\"\n",
    "graphdir = \"../semEval16/graph_parser/predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73600175-6a33-40c5-8c94-fe735576e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'test-B', 'test-gold']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00689041-af4f-4455-b2d3-b0eeae096ad2",
   "metadata": {},
   "source": [
    "## 0.1 Check if test-B and test-gold are the same splits -> Yes\n",
    "\n",
    "- Ans: Yes, review_id和sent_id都是同一個set。\n",
    "- test-gold才是有完整答案的（全部都有標注除了intensity）\n",
    "- test-B缺標polarity（缺標polarity和intensity）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3926b73-3275-4bbb-a1ac-acb8ceae5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../semEval16/data/train/ABSA16_Restaurants_Train_SB1_v2.json\n",
      "Loading from ../semEval16/data/test-B/EN_REST_SB1_TEST-B.json\n",
      "Loading from ../semEval16/data/test-gold/EN_REST_SB1_TEST-gold.json\n"
     ]
    }
   ],
   "source": [
    "splitpaths = [f'{datadir}/train/ABSA16_Restaurants_Train_SB1_v2.json',\n",
    "    f'{datadir}/test-B/EN_REST_SB1_TEST-B.json',\n",
    "    f'{datadir}/test-gold/EN_REST_SB1_TEST-gold.json']\n",
    "\n",
    "\n",
    "import json \n",
    "\n",
    "splitD = {'train':None, 'test-B':None, 'test-gold':None}\n",
    "for split, sppath in zip(splits, splitpaths):\n",
    "    print('Loading from', sppath)\n",
    "    with open(sppath, 'r') as f:\n",
    "        splitD[split] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82a19995-6e6e-4b7e-b64f-0496e2f46a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bset = set(x['sent_id'] for x in splitD['test-B'])\n",
    "Gset = set(x['sent_id'] for x in splitD['test-gold']) \n",
    "Bset == Gset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c63b05f1-aa49-4341-a866-a60f6f273f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Bset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b9f1139-344b-4947-a400-2d4d8de4d16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': 'en_OpenSesame_477970940',\n",
       " 'sent_id': 'en_OpenSesame_477970940:5',\n",
       " 'text': 'Also, they serve THE best hummus in America, with a drizzle of fragrant olive oil (which, I believe is the traditional way)!',\n",
       " 'opinions': [{'Source': [[], []],\n",
       "   'Target': ['hummus', ['26:32']],\n",
       "   'Polar_expression': [[], []],\n",
       "   'Polarity': 'positive',\n",
       "   'Intensity': '',\n",
       "   'Category': 'FOOD#QUALITY'},\n",
       "  {'Source': [[], []],\n",
       "   'Target': ['hummus', ['26:32']],\n",
       "   'Polar_expression': [[], []],\n",
       "   'Polarity': 'positive',\n",
       "   'Intensity': '',\n",
       "   'Category': 'FOOD#STYLE_OPTIONS'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 'en_BlueRibbonSushi_478218171'\n",
    "from random import randint\n",
    "rindex = randint(0, len(Gset)) \n",
    "splitD['test-gold'][rindex] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bf91f23-59ff-4970-8c9c-4301a5589e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review_id': 'en_OpenSesame_477970940',\n",
       " 'sent_id': 'en_OpenSesame_477970940:5',\n",
       " 'text': 'Also, they serve THE best hummus in America, with a drizzle of fragrant olive oil (which, I believe is the traditional way)!',\n",
       " 'opinions': [{'Source': [[], []],\n",
       "   'Target': ['hummus', ['26:32']],\n",
       "   'Polar_expression': [[], []],\n",
       "   'Polarity': '',\n",
       "   'Intensity': '',\n",
       "   'Category': 'FOOD#QUALITY'},\n",
       "  {'Source': [[], []],\n",
       "   'Target': ['hummus', ['26:32']],\n",
       "   'Polar_expression': [[], []],\n",
       "   'Polarity': '',\n",
       "   'Intensity': '',\n",
       "   'Category': 'FOOD#STYLE_OPTIONS'}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitD['test-B'][rindex]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2dc13-8455-4f85-8868-c5e943551880",
   "metadata": {},
   "source": [
    "## 1 eval metrics\n",
    "   - `Target`\n",
    "       試著detect相同或相近target的opinion，將gold_target加入預測，找不到則填''。\n",
    "   - `(Target, polarity)`\n",
    "       試著detect相同或相近target的opinion。\n",
    "       找到對應predicted polarity，將f'{gold_target}_{polarity}'加入預測，找不到則填null_null。\n",
    "   - 計算f1時以tuple為單位，傳入的sklearn.metrics.f1_score(y_true, y_pred)之y_true長度應為整個dataset中gold tuples總數。\n",
    "   - note that polarity is to be cast to uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b12e6c-702d-429a-a3e1-58a976f6bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'test-gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "199f2de6-c60e-4ec1-8357-10818de73571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### loading predictions \n",
    "from collections import defaultdict\n",
    "predD = defaultdict(lambda: defaultdict(dict))\n",
    "# 3 dataset-pretrained models \n",
    "prt_dataset = ['darmstadt_unis', 'mpqa', 'opener_en']   \n",
    "for i, baselinedir in enumerate([seqdir, graphdir]):\n",
    "    baseline = 'seq' if i == 0 else 'graph'\n",
    "    for split in splits:\n",
    "        for pretrained_d in prt_dataset:\n",
    "            fullpath = f'{baselinedir}/{split}/{pretrained_d}_prediction.json'\n",
    "            with open(fullpath, 'r') as f:\n",
    "                predD[baseline][split][pretrained_d] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db1ee445-1bc3-45a2-b769-dc73d7ac534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  predD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb592779-d9d2-419a-b087-43970eb871e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the same sent_id, \n",
    "# run through gold targets \n",
    " # run through pred targets \n",
    "    # how to ignore duplicates?\n",
    "        # if lcs(gold, pred) >= 4\n",
    "         # consider them to be correct and append it to the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34908b9d-e748-4c2b-b529-de6c7b271698",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursplit = 'train'\n",
    "baseline = 'graph'\n",
    "pretrained = 'opener_en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcfbe8bb-9e78-4491-86d7-181cfbb782d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden = splitD[cursplit]\n",
    "prediction = predD[baseline][cursplit][pretrained]\n",
    "pairing = defaultdict(lambda: {'gold':None, 'pred':None})\n",
    "for y in golden:\n",
    "    sid = y['sent_id']\n",
    "    pairing[sid]['gold'] = y \n",
    "for y_ in prediction:\n",
    "    sid = y_['sent_id']\n",
    "    pairing[sid]['pred'] = y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b47e51f-189e-4fa7-9aca-303af8d515cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to: https://www.geeksforgeeks.org/longest-common-substring-dp-29/\n",
    "def LCSubStr(X, Y):\n",
    "    m, n = len(X), len(Y)\n",
    "    # Create a table to store lengths of\n",
    "    # longest common suffixes of substrings.\n",
    "    # Note that LCSuff[i][j] contains the\n",
    "    # length of longest common suffix of\n",
    "    # X[0...i-1] and Y[0...j-1]. The first\n",
    "    # row and first column entries have no\n",
    "    # logical meaning, they are used only\n",
    "    # for simplicity of the program.\n",
    " \n",
    "    # LCSuff is the table with zero\n",
    "    # value initially in each cell\n",
    "    LCSuff = [[0 for k in range(n+1)] for l in range(m+1)]\n",
    " \n",
    "    # To store the length of\n",
    "    # longest common substring\n",
    "    result = 0\n",
    " \n",
    "    # Following steps to build\n",
    "    # LCSuff[m+1][n+1] in bottom up fashion\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if (i == 0 or j == 0):\n",
    "                LCSuff[i][j] = 0\n",
    "            elif (X[i-1] == Y[j-1]):\n",
    "                LCSuff[i][j] = LCSuff[i-1][j-1] + 1\n",
    "                result = max(result, LCSuff[i][j])\n",
    "            else:\n",
    "                LCSuff[i][j] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ec755e4d-2d03-4f7d-81fa-54a4cf698ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_targets(data):\n",
    "    opns = data['opinions']\n",
    "    x = set()\n",
    "    for opn in opns:\n",
    "        if isinstance(opn['Target'][0], str):\n",
    "            x.add(opn['Target'][0])\n",
    "        elif len(opn['Target'][0]) > 0:\n",
    "            x.add(opn['Target'][0][0])\n",
    "    return list(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b07cd7-a767-4ef7-a42f-e7184a16e191",
   "metadata": {},
   "source": [
    "### 1.1 target eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9c2109bf-9a3d-4bdd-8240-d4deedd62364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_scorer(pairing):\n",
    "    '''\n",
    "    pairing: defaultdict(lambda: {'gold':None, 'pred':None})\n",
    "        gold_data = pairing[sent_id]['gold']\n",
    "        pred_data = pairing[sent_id]['pred']\n",
    "    '''\n",
    "    import re\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    goldtargets = [] # hold gold targets\n",
    "    predtargets = [] # hold preprocessed pred targets\n",
    "    correct = {'exact':0, 'lcs':0} \n",
    "    for k, pair in pairing.items():\n",
    "\n",
    "        goldtgts = collect_targets(pair['gold'])\n",
    "        predtgts = collect_targets(pair['pred'])\n",
    "        # print(goldtgts, predtgts)\n",
    "        for goldtarget in goldtgts:\n",
    "            goldtarget = re.sub(r\"[,.;@#?!&$]+\\ *\", \"\", goldtarget)\n",
    "            goldtargets.append(goldtarget)\n",
    "\n",
    "            addFlag = False\n",
    "            for pi, predtarget in enumerate(predtgts):\n",
    "                predtarget = re.sub(r\"[,.;@#?!&$]+\\ *\", \"\", predtarget)\n",
    "                # check lcs (longest common substring)\n",
    "                if goldtarget == predtarget:\n",
    "                    predtargets.append(goldtarget)\n",
    "                    predtgts.pop(pi)\n",
    "                    addFlag = True\n",
    "                    correct['exact'] += 1\n",
    "                    break \n",
    "                if LCSubStr(goldtarget, predtarget) >= 4:\n",
    "                    # print(goldtarget, '|', predtarget)\n",
    "                    predtargets.append(goldtarget)\n",
    "                    predtgts.pop(pi)\n",
    "                    addFlag = True\n",
    "                    correct['lcs'] += 1\n",
    "                    break\n",
    "            if not addFlag:\n",
    "                predtargets.append('')\n",
    "    assert len(goldtargets) == len(predtargets)\n",
    "    print('='*7+' TARGET SCORER '+'='*7)\n",
    "    print(f'- # sentences: {len(pairing)}')\n",
    "    print(f'- # gold tuples: {len(goldtargets)}')\n",
    "    print(f'- # correct: {correct}')\n",
    "    f1 = f1_score(goldtargets, predtargets, average=\"micro\")\n",
    "    print(f'- f1 score (micro): {f1}')\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dcce8c07-47ad-450c-866a-d4583f88c360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= TARGET SCORER =======\n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1741\n",
      "- # correct: {'exact': 282, 'lcs': 479}\n",
      "- f1 score (micro): 0.43710511200459506\n"
     ]
    }
   ],
   "source": [
    "target_scorer(pairing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c472a-8a16-4fdc-84a7-2a01e69a688b",
   "metadata": {},
   "source": [
    "### 1.2 (target, pol) eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "532861ae-b55d-49b3-839e-89603641311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tuples(data):\n",
    "    opns = data['opinions']\n",
    "    x = set()\n",
    "    for opn in opns:\n",
    "        if isinstance(opn['Target'][0], str):\n",
    "            x.add((opn['Target'][0], opn['Polarity'].lower()))\n",
    "        elif len(opn['Target'][0]) > 0:\n",
    "            x.add((opn['Target'][0][0], opn['Polarity'].lower()))\n",
    "    return list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ae367ec7-4a0a-484d-a694-ef9df3fd8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_scorer(pairing):\n",
    "    '''\n",
    "    pairing: defaultdict(lambda: {'gold':None, 'pred':None})\n",
    "        gold_data = pairing[sent_id]['gold']\n",
    "        pred_data = pairing[sent_id]['pred']\n",
    "    '''\n",
    "    import re\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    goldtuples = [] # hold gold targets\n",
    "    predtuples = [] # hold preprocessed pred targets\n",
    "\n",
    "    for k, pair in pairing.items():\n",
    "\n",
    "        goldtups = collect_tuples(pair['gold'])\n",
    "        predtups = collect_tuples(pair['pred'])\n",
    "        for goldtarget, goldpol in goldtups:\n",
    "            goldtarget = re.sub(r\"[,.;@#?!&$]+\\ *\", \"\", goldtarget)\n",
    "            goldtuples.append(f'{goldtarget}_{goldpol}')\n",
    "\n",
    "            addFlag = False\n",
    "            for pi, (predtarget, predpol) in enumerate(predtups):\n",
    "                predtarget = re.sub(r\"[,.;@#?!&$]+\\ *\", \"\", predtarget)\n",
    "                # check lcs (longest common substring)\n",
    "                if goldtarget == predtarget or LCSubStr(goldtarget, predtarget) >= 4:\n",
    "                    predtuples.append(f'{goldtarget}_{predpol}')\n",
    "                    predtups.pop(pi)\n",
    "                    addFlag = True\n",
    "                    break\n",
    "            if not addFlag:\n",
    "                predtuples.append('null_null')\n",
    "    \n",
    "    \n",
    "    assert len(goldtuples) == len(predtuples)\n",
    "    print('='*7+' TUPLE SCORER '+'='*7)\n",
    "    print('- TUPLE = (target, polairty) ')\n",
    "    print(f'- # sentences: {len(pairing)}')\n",
    "    print(f'- # gold tuples: {len(goldtuples)}')\n",
    "    # print(goldtuples[:10], predtuples[:10])\n",
    "    f1 = f1_score(goldtuples, predtuples, average=\"micro\")\n",
    "    print(f'- f1 score (micro): {f1}')\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9478d9a5-29f0-40b9-bfab-f510e11285a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1770\n",
      "- f1 score (micro): 0.3338983050847458\n"
     ]
    }
   ],
   "source": [
    "tuple_scorer(pairing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b0d6f-e0d2-4083-a0da-07d7f7875f0e",
   "metadata": {},
   "source": [
    "## 2 Compute scores for each annotation \n",
    "   \n",
    "   - 2 baselines x 2 pretrained datasets x 2 splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5a3042c5-2a7a-40f5-9f9c-9b0447c4cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makepair(G, P):\n",
    "    pairing = defaultdict(lambda: {'gold':None, 'pred':None})\n",
    "    for y in G:\n",
    "        sid = y['sent_id']\n",
    "        pairing[sid]['gold'] = y \n",
    "    for y_ in P:\n",
    "        sid = y_['sent_id']\n",
    "        pairing[sid]['pred'] = y_\n",
    "    del G, P\n",
    "    return pairing             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dade3d-0720-4141-b197-28bde8ac0da8",
   "metadata": {},
   "source": [
    "### 2.1 score log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "cf9b0a29-46d8-4d0a-ae17-eb1de2f16888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOW EVALUATING seq_train_darmstadt_unis...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1741\n",
      "- # correct: {'exact': 146, 'lcs': 51}\n",
      "- f1 score (micro): 0.1131533601378518\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1770\n",
      "- f1 score (micro): 0.07909604519774012\n",
      "\n",
      "NOW EVALUATING seq_train_mpqa...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1741\n",
      "- # correct: {'exact': 3, 'lcs': 22}\n",
      "- f1 score (micro): 0.014359563469270534\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1770\n",
      "- f1 score (micro): 0.005084745762711864\n",
      "\n",
      "NOW EVALUATING seq_train_opener_en...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1741\n",
      "- # correct: {'exact': 235, 'lcs': 508}\n",
      "- f1 score (micro): 0.4267662263067203\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1770\n",
      "- f1 score (micro): 0.3135593220338983\n",
      "\n",
      "NOW EVALUATING graph_train_darmstadt_unis...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1741\n",
      "- # correct: {'exact': 184, 'lcs': 18}\n",
      "- f1 score (micro): 0.11602527283170591\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1770\n",
      "- f1 score (micro): 0.06327683615819209\n",
      "\n",
      "NOW EVALUATING graph_train_mpqa...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1741\n",
      "- # correct: {'exact': 2, 'lcs': 36}\n",
      "- f1 score (micro): 0.021826536473291212\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1770\n",
      "- f1 score (micro): 0.012994350282485875\n",
      "\n",
      "NOW EVALUATING graph_train_opener_en...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1741\n",
      "- # correct: {'exact': 282, 'lcs': 479}\n",
      "- f1 score (micro): 0.43710511200459506\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 2000\n",
      "- # gold tuples: 1770\n",
      "- f1 score (micro): 0.3338983050847458\n",
      "\n",
      "NOW EVALUATING seq_test-gold_darmstadt_unis...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 676\n",
      "- # gold tuples: 612\n",
      "- # correct: {'exact': 54, 'lcs': 14}\n",
      "- f1 score (micro): 0.1111111111111111\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 676\n",
      "- # gold tuples: 623\n",
      "- f1 score (micro): 0.0738362760834671\n",
      "\n",
      "NOW EVALUATING seq_test-gold_mpqa...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 676\n",
      "- # gold tuples: 612\n",
      "- # correct: {'exact': 1, 'lcs': 11}\n",
      "- f1 score (micro): 0.0196078431372549\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 676\n",
      "- # gold tuples: 623\n",
      "- f1 score (micro): 0.006420545746388443\n",
      "\n",
      "NOW EVALUATING seq_test-gold_opener_en...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 676\n",
      "- # gold tuples: 612\n",
      "- # correct: {'exact': 77, 'lcs': 167}\n",
      "- f1 score (micro): 0.39869281045751637\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 676\n",
      "- # gold tuples: 623\n",
      "- f1 score (micro): 0.2825040128410915\n",
      "\n",
      "NOW EVALUATING graph_test-gold_darmstadt_unis...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 676\n",
      "- # gold tuples: 612\n",
      "- # correct: {'exact': 46, 'lcs': 10}\n",
      "- f1 score (micro): 0.0915032679738562\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 676\n",
      "- # gold tuples: 623\n",
      "- f1 score (micro): 0.052969502407704656\n",
      "\n",
      "NOW EVALUATING graph_test-gold_mpqa...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 676\n",
      "- # gold tuples: 612\n",
      "- # correct: {'exact': 1, 'lcs': 19}\n",
      "- f1 score (micro): 0.032679738562091505\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 676\n",
      "- # gold tuples: 623\n",
      "- f1 score (micro): 0.014446227929373997\n",
      "\n",
      "NOW EVALUATING graph_test-gold_opener_en...\n",
      "\n",
      "======= TARGET SCORER =======\n",
      "- # sentences: 676\n",
      "- # gold tuples: 612\n",
      "- # correct: {'exact': 90, 'lcs': 157}\n",
      "- f1 score (micro): 0.4035947712418301\n",
      "======= TUPLE SCORER =======\n",
      "- TUPLE = (target, polairty) \n",
      "- # sentences: 676\n",
      "- # gold tuples: 623\n",
      "- f1 score (micro): 0.33226324237560195\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "splits = ['train', 'test-gold']\n",
    "splitstats = {}\n",
    "for split in splits:\n",
    "    goldfile = splitD[split]\n",
    "    seqstats = pd.Series(index=prt_dataset, name = 'seq_labeling', dtype = object)\n",
    "    graphstats = pd.Series(index=prt_dataset, name = 'graph_parser', dtype = object)\n",
    "    for i, baselinedir in enumerate([seqdir, graphdir]):\n",
    "        baseline = 'seq' if i == 0 else 'graph'\n",
    "        for pretrained in prt_dataset:\n",
    "            predfile = predD[baseline][split][pretrained]\n",
    "            # making pairing dictionary \n",
    "            print(f'\\nNOW EVALUATING {baseline}_{split}_{pretrained}...\\n')\n",
    "            pairing = makepair(goldfile, predfile)\n",
    "            tgtf1 = round(target_scorer(pairing), 4)\n",
    "            tupf1 = round(tuple_scorer(pairing), 4) \n",
    "            if i == 0:\n",
    "                seqstats[pretrained] = (tgtf1, tupf1)\n",
    "            else: graphstats[pretrained] = (tgtf1, tupf1)\n",
    "    splitstats[split] = pd.concat([seqstats, graphstats], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a7aef-6086-48bc-b71e-0b1252e74ff3",
   "metadata": {},
   "source": [
    "### 2.2 score tables\n",
    "score description: `(target micro f1, target_polarity tuple micro f1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "66b865b8-7324-4d76-a950-49a9fabd5fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_labeling</th>\n",
       "      <th>graph_parser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>darmstadt_unis</th>\n",
       "      <td>(0.1132, 0.0791)</td>\n",
       "      <td>(0.116, 0.0633)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpqa</th>\n",
       "      <td>(0.0144, 0.0051)</td>\n",
       "      <td>(0.0218, 0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opener_en</th>\n",
       "      <td>(0.4268, 0.3136)</td>\n",
       "      <td>(0.4371, 0.3339)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    seq_labeling      graph_parser\n",
       "darmstadt_unis  (0.1132, 0.0791)   (0.116, 0.0633)\n",
       "mpqa            (0.0144, 0.0051)   (0.0218, 0.013)\n",
       "opener_en       (0.4268, 0.3136)  (0.4371, 0.3339)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitstats['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a57fcc27-ecaf-44b1-9795-a789a0b1d24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_labeling</th>\n",
       "      <th>graph_parser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>darmstadt_unis</th>\n",
       "      <td>(0.1111, 0.0738)</td>\n",
       "      <td>(0.0915, 0.053)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpqa</th>\n",
       "      <td>(0.0196, 0.0064)</td>\n",
       "      <td>(0.0327, 0.0144)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opener_en</th>\n",
       "      <td>(0.3987, 0.2825)</td>\n",
       "      <td>(0.4036, 0.3323)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    seq_labeling      graph_parser\n",
       "darmstadt_unis  (0.1111, 0.0738)   (0.0915, 0.053)\n",
       "mpqa            (0.0196, 0.0064)  (0.0327, 0.0144)\n",
       "opener_en       (0.3987, 0.2825)  (0.4036, 0.3323)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitstats['test-gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae68fe-3106-4c69-a0a7-7650d4e8c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
